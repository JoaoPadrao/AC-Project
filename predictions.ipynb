{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.read_csv('data/selection/selected_features_teams.csv')\n",
    "players = pd.read_csv('data/clean/cleaned_players.csv')\n",
    "players_teams = pd.read_csv('data/clean/cleaned_players_teams.csv')\n",
    "coaches = pd.read_csv('data/clean/cleaned_coaches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coach_experience_for_team(coaches, team_id, year):\n",
    "    team_coaches = coaches[(coaches['tmID'] == team_id) & (coaches['year'] == year)]\n",
    "    total_games = team_coaches['won'].sum() + team_coaches['lost'].sum()\n",
    "    \n",
    "    total_coach_experience = 0\n",
    "    \n",
    "    for _, coach in team_coaches.iterrows():\n",
    "        coach_history = coaches[(coaches['coachID'] == coach['coachID']) & (coaches['year'] < year)]\n",
    "        coach_history = coach_history.sort_values(by='year', ascending=False).head(year)\n",
    "\n",
    "        weights = list(range(year, 0, -1)) \n",
    "        weighted_winrate = sum(coach_history['winrate'] * weights[:len(coach_history)])\n",
    "        total_awards = coach_history['TotalAwards'].sum()\n",
    "        coach_experience = weighted_winrate + total_awards\n",
    "        \n",
    "        coach_games = coach['won'] + coach['lost']\n",
    "        coach_weight = coach_games / total_games if total_games > 0 else 0\n",
    "        total_coach_experience += coach_experience * coach_weight\n",
    "    \n",
    "    return total_coach_experience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Team Year Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No stats for player coggicl01w\n",
      "No stats for player joensca01w\n",
      "No stats for player pricear01w\n",
      "No stats for player raymost01w\n",
      "No stats for player thomaca01w\n",
      "No stats for player chambco01w\n",
      "No stats for player hairska01w\n",
      "No stats for player maltsev01w\n",
      "No stats for player lattaiv01w\n",
      "No stats for player sandeam01w\n",
      "No stats for player smithty01w\n",
      "No stats for player davisde01w\n",
      "No stats for player granter01w\n",
      "No stats for player shielas01w\n",
      "No stats for player balesal01w\n",
      "No stats for player fernama01w\n",
      "No stats for player spencsi01w\n",
      "No stats for player hardili01w\n",
      "No stats for player moorena01w\n",
      "No stats for player murphsh01w\n",
      "No stats for player quinnno01w\n",
      "No stats for player bowenli01w\n",
      "No stats for player davenje01w\n",
      "No stats for player doronsh01w\n",
      "No stats for player jacksti02w\n",
      "No stats for player weberma01w\n",
      "No stats for player littlca01w\n",
      "No stats for player gearlka01w\n",
      "No stats for player goringi01w\n",
      "No stats for player mosbybe01w\n"
     ]
    }
   ],
   "source": [
    "def predict_team_year_stats(team_id, year): \n",
    "\n",
    "    predicted_stats = teams.loc[(teams['tmID'] == team_id) & (teams['year'] == year - 1)].copy()\n",
    "    predicted_stats['year'] = year\n",
    "\n",
    "    if teams[(teams['tmID'] == team_id) & (teams['year'] == year - 1)].empty:\n",
    "       print(f\"Team {team_id} is new in year {year}. Using average rookie team stats.\")\n",
    "       predicted_stats = teams.loc[(teams['tmID'] == 'average_rookie_team')].copy()\n",
    "       predicted_stats['year'] = year\n",
    "       predicted_stats['franchID'] = teams[teams['tmID'] == team_id]['franchID'].iloc[0]\n",
    "       predicted_stats['year'] = year\n",
    "       predicted_stats['tmID'] = team_id\n",
    "       print(predicted_stats)\n",
    "\n",
    "    # Select player ids for the team for that year\n",
    "    players_ids = players_teams[(players_teams['tmID'] == team_id) & (players_teams['year'] == year)]['playerID']\n",
    "    # Select player stats for last year\n",
    "    team_players = players_teams[(players_teams['playerID'].isin(players_ids)) & (players_teams['year'] == year - 1)]\n",
    "    team_players_bio = players[(players['bioID'].isin(players_ids))]\n",
    "    \n",
    "    # Selecionar stats para jogadores considerando o ano mais recente possível\n",
    "    team_players = []\n",
    "    for player_id in players_ids:\n",
    "        player_stats = players_teams[players_teams['playerID'] == player_id]\n",
    "        if not player_stats.empty:\n",
    "            recent_stats = player_stats[player_stats['year'] < year].sort_values('year', ascending=False).head(1)\n",
    "            if not recent_stats.empty:\n",
    "                team_players.append(recent_stats)\n",
    "            else:\n",
    "                print(f'No stats for player {player_id}')\n",
    "                team_players.append(players_teams[players_teams['playerID'] == 'average_rookie'])         \n",
    "\n",
    "\n",
    "    \n",
    "    # Combinar as estatísticas de todos os jogadores\n",
    "    team_players = pd.concat(team_players, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # Calculate the player stats\n",
    "    predicted_stats['player_average_height'] = team_players_bio['height'].mean()\n",
    "    predicted_stats['player_average_weight'] = team_players_bio['weight'].mean()\n",
    "    predicted_stats['player_total_GP'] = team_players['GP'].sum()\n",
    "    predicted_stats['player_total_GS'] = team_players['GS'].sum()\n",
    "    predicted_stats['player_total_points'] = team_players['points'].sum()\n",
    "    predicted_stats['player_total_oRebounds'] = team_players['oRebounds'].sum()\n",
    "    predicted_stats['player_total_dRebounds'] = team_players['dRebounds'].sum()\n",
    "    predicted_stats['player_total_rebounds'] = team_players['rebounds'].sum()\n",
    "    predicted_stats['player_total_assists'] = team_players['assists'].sum()\n",
    "    predicted_stats['player_total_steals'] = team_players['steals'].sum()\n",
    "    predicted_stats['player_total_blocks'] = team_players['blocks'].sum()\n",
    "    predicted_stats['player_total_turnovers'] = team_players['turnovers'].sum()\n",
    "    predicted_stats['player_total_PF'] = team_players['PF'].sum()\n",
    "    predicted_stats['player_total_fgAttempted'] = team_players['fgAttempted'].sum()\n",
    "    predicted_stats['player_total_fgMade'] = team_players['fgMade'].sum()\n",
    "    predicted_stats['player_total_ftAttempted'] = team_players['ftAttempted'].sum()\n",
    "    predicted_stats['player_total_ftMade'] = team_players['ftMade'].sum()\n",
    "    predicted_stats['player_total_threeAttempted'] = team_players['threeAttempted'].sum()\n",
    "    predicted_stats['player_total_threeMade'] = team_players['threeMade'].sum()\n",
    "    predicted_stats['player_total_dq'] = team_players['dq'].sum()\n",
    "    predicted_stats['player_total_PostGP'] = team_players['PostGP'].sum()\n",
    "    predicted_stats['player_total_PostGS'] = team_players['PostGS'].sum()\n",
    "    predicted_stats['player_total_PostMinutes'] = team_players['PostMinutes'].sum()\n",
    "    predicted_stats['player_total_PostPoints'] = team_players['PostPoints'].sum()\n",
    "    predicted_stats['player_total_PostoRebounds'] = team_players['PostoRebounds'].sum()\n",
    "    predicted_stats['player_total_PostdRebounds'] = team_players['PostdRebounds'].sum()\n",
    "    predicted_stats['player_total_PostRebounds'] = team_players['PostRebounds'].sum()\n",
    "    predicted_stats['player_total_PostAssists'] = team_players['PostAssists'].sum()\n",
    "    predicted_stats['player_total_PostSteals'] = team_players['PostSteals'].sum()\n",
    "    predicted_stats['player_total_PostBlocks'] = team_players['PostBlocks'].sum()\n",
    "    predicted_stats['player_total_PostTurnovers'] = team_players['PostTurnovers'].sum()\n",
    "    predicted_stats['player_total_PostPF'] = team_players['PostPF'].sum()\n",
    "    predicted_stats['player_total_PostfgAttempted'] = team_players['PostfgAttempted'].sum()\n",
    "    predicted_stats['player_total_PostfgMade'] = team_players['PostfgMade'].sum()\n",
    "    predicted_stats['player_total_PostftAttempted'] = team_players['PostftAttempted'].sum()\n",
    "    predicted_stats['player_total_PostftMade'] = team_players['PostftMade'].sum()\n",
    "    predicted_stats['player_total_PostthreeAttempted'] = team_players['PostthreeAttempted'].sum()\n",
    "    predicted_stats['player_total_PostthreeMade'] = team_players['PostthreeMade'].sum()\n",
    "    predicted_stats['player_total_PostDQ'] = team_players['PostDQ'].sum()\n",
    "    predicted_stats['player_total_awards'] = team_players['TotalAwards'].sum()\n",
    "    \n",
    "    coach_experience = calculate_coach_experience_for_team(coaches, team_id, year)\n",
    "    predicted_stats['coach_experience'] = coach_experience\n",
    "\n",
    "    \n",
    "    predicted_stats['playoff'] = \"\"\n",
    "    predicted_stats['firstRound'] = \"\"\n",
    "    predicted_stats['semis'] = \"\"\n",
    "    predicted_stats['finals'] = \"\"\n",
    "    \n",
    "    return predicted_stats\n",
    "\n",
    "\n",
    "# Function that returns a dataframe with all team stats for every year from 1 to year-1 plus the predicted stats for year\n",
    "def get_year_predictions(year):\n",
    "    team_predictions = []  # Use a list to collect rows\n",
    "    for index, row in teams.iterrows():\n",
    "        if row['year'] < year:\n",
    "            team_predictions.append(\n",
    "                teams.loc[(teams['tmID'] == row['tmID']) & (teams['year'] == row['year'])]\n",
    "            )\n",
    "        elif row['year'] == year:\n",
    "            predicted_stats = predict_team_year_stats(row['tmID'], year)\n",
    "            team_predictions.append(predicted_stats)\n",
    "            \n",
    "    return pd.concat(team_predictions, ignore_index=True)\n",
    "\n",
    "# save 10 year\n",
    "\n",
    "# Get the predictions for year 10 and save them to data/clean/year_7_predictions.csv\n",
    "year_10_predictions = get_year_predictions(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data:  (116, 75)\n",
      "Model: Random Forest Classifier\n",
      "Accuracy:  0.5384615384615384\n",
      "Precision:  0.6\n",
      "Recall:  0.75\n",
      "F1 Score:  0.6666666666666666\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.33      0.20      0.25         5\n",
      "           Y       0.60      0.75      0.67         8\n",
      "\n",
      "    accuracy                           0.54        13\n",
      "   macro avg       0.47      0.47      0.46        13\n",
      "weighted avg       0.50      0.54      0.51        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "print(\"Shape of the data: \", year_10_predictions.shape)\n",
    "train_data = year_10_predictions[year_10_predictions['year'] <= 7].dropna(subset=['playoff'])\n",
    "X = train_data.drop(columns=['playoff', 'year', 'tmID', 'franchID', 'confID', 'firstRound', 'semis', 'finals', 'rank'])\n",
    "y = train_data['playoff'] \n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "year_10_data = year_10_predictions[year_10_predictions['year'] == 8].drop(columns=['playoff', 'year', 'tmID', 'franchID', 'confID', 'firstRound', 'semis', 'finals', 'rank'])\n",
    "predictions = model.predict(year_10_data)\n",
    "\n",
    "year_10_predictions.loc[year_10_predictions['year'] == 8, 'playoff'] = predictions\n",
    "\n",
    "real_values = teams[teams['year'] == 8]['playoff']\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(real_values, predictions)\n",
    "precision = precision_score(real_values, predictions, pos_label='Y')  \n",
    "recall = recall_score(real_values, predictions, pos_label='Y')\n",
    "f1 = f1_score(real_values, predictions, pos_label='Y')\n",
    "\n",
    "print(\"Model: Random Forest Classifier\")\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(real_values, predictions, target_names=['N', 'Y']))\n",
    "\n",
    "year_10_predictions.to_csv('data/clean/year_10_predictions1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.6153846153846154\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.75\n",
      "F1 Score: 0.7058823529411765\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.50      0.40      0.44         5\n",
      "           Y       0.67      0.75      0.71         8\n",
      "\n",
      "    accuracy                           0.62        13\n",
      "   macro avg       0.58      0.57      0.58        13\n",
      "weighted avg       0.60      0.62      0.61        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "train_data = year_10_predictions[year_10_predictions['year'] <= 9].dropna(subset=['playoff'])\n",
    "X_train = train_data.drop(columns=['playoff', 'year', 'tmID', 'franchID', 'confID', 'firstRound', 'semis', 'finals', 'rank'])\n",
    "y_train = train_data['playoff']\n",
    "\n",
    "logreg_model = LogisticRegression(random_state=42)\n",
    "\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "X_test = year_10_predictions[year_10_predictions['year'] == 10].drop(columns=['playoff', 'year', 'tmID', 'franchID', 'confID', 'firstRound', 'semis', 'finals', 'rank'])\n",
    "real_values = teams[teams['year'] == 10]['playoff']\n",
    "\n",
    "X_test = X_test[X_train.columns]  \n",
    "\n",
    "logreg_predictions = logreg_model.predict(X_test)\n",
    "\n",
    "logreg_accuracy = accuracy_score(real_values, logreg_predictions)\n",
    "logreg_precision = precision_score(real_values, logreg_predictions, pos_label='Y')\n",
    "logreg_recall = recall_score(real_values, logreg_predictions, pos_label='Y')\n",
    "logreg_f1 = f1_score(real_values, logreg_predictions, pos_label='Y')\n",
    "\n",
    "print(\"Model: Logistic Regression\")\n",
    "print(f\"Accuracy: {logreg_accuracy}\")\n",
    "print(f\"Precision: {logreg_precision}\")\n",
    "print(f\"Recall: {logreg_recall}\")\n",
    "print(f\"F1 Score: {logreg_f1}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(real_values, logreg_predictions, target_names=['N', 'Y']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.6153846153846154\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.75\n",
      "F1 Score: 0.7058823529411765\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.50      0.40      0.44         5\n",
      "           Y       0.67      0.75      0.71         8\n",
      "\n",
      "    accuracy                           0.62        13\n",
      "   macro avg       0.58      0.57      0.58        13\n",
      "weighted avg       0.60      0.62      0.61        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "train_data = year_10_predictions[year_10_predictions['year'] <= 9].dropna(subset=['playoff'])\n",
    "X_train = train_data.drop(columns=['playoff', 'year', 'tmID', 'franchID', 'confID', 'firstRound', 'semis', 'finals', 'rank'])\n",
    "y_train = train_data['playoff']\n",
    "\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "X_test = year_10_predictions[year_10_predictions['year'] == 10].drop(columns=['playoff', 'year', 'tmID', 'franchID', 'confID', 'firstRound', 'semis', 'finals', 'rank'])\n",
    "real_values = teams[teams['year'] == 10]['playoff']\n",
    "\n",
    "X_test = X_test[X_train.columns]  \n",
    "\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "svm_accuracy = accuracy_score(real_values, svm_predictions)\n",
    "svm_precision = precision_score(real_values, svm_predictions, pos_label='Y')\n",
    "svm_recall = recall_score(real_values, svm_predictions, pos_label='Y')\n",
    "svm_f1 = f1_score(real_values, svm_predictions, pos_label='Y')\n",
    "\n",
    "print(\"\\nModel: Support Vector Machine\")\n",
    "print(f\"Accuracy: {svm_accuracy}\")\n",
    "print(f\"Precision: {svm_precision}\")\n",
    "print(f\"Recall: {svm_recall}\")\n",
    "print(f\"F1 Score: {svm_f1}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(real_values, svm_predictions, target_names=['N', 'Y']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: KNN (K-Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 0.5384615384615384\n",
      "Precision: 0.625\n",
      "Recall: 0.625\n",
      "F1 Score: 0.625\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.40      0.40      0.40         5\n",
      "           Y       0.62      0.62      0.62         8\n",
      "\n",
      "    accuracy                           0.54        13\n",
      "   macro avg       0.51      0.51      0.51        13\n",
      "weighted avg       0.54      0.54      0.54        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "train_data = year_10_predictions[year_10_predictions['year'] <= 9].dropna(subset=['playoff'])\n",
    "X_train = train_data.drop(columns=['playoff', 'year', 'tmID', 'franchID', 'confID', 'firstRound', 'semis', 'finals', 'rank'])\n",
    "y_train = train_data['playoff']\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "X_test = year_10_predictions[year_10_predictions['year'] == 10].drop(columns=['playoff', 'year', 'tmID', 'franchID', 'confID', 'firstRound', 'semis', 'finals', 'rank'])\n",
    "real_values = teams[teams['year'] == 10]['playoff']\n",
    "\n",
    "X_test = X_test[X_train.columns] \n",
    "\n",
    "knn_predictions = knn_model.predict(X_test)\n",
    "\n",
    "knn_accuracy = accuracy_score(real_values, knn_predictions)\n",
    "knn_precision = precision_score(real_values, knn_predictions, pos_label='Y')\n",
    "knn_recall = recall_score(real_values, knn_predictions, pos_label='Y')\n",
    "knn_f1 = f1_score(real_values, knn_predictions, pos_label='Y')\n",
    "\n",
    "print(\"\\nModel: K-Nearest Neighbors\")\n",
    "print(f\"Accuracy: {knn_accuracy}\")\n",
    "print(f\"Precision: {knn_precision}\")\n",
    "print(f\"Recall: {knn_recall}\")\n",
    "print(f\"F1 Score: {knn_f1}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(real_values, knn_predictions, target_names=['N', 'Y']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.5384615384615384\n",
      "Precision: 0.6\n",
      "Recall: 0.75\n",
      "F1 Score: 0.6666666666666666\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.33      0.20      0.25         5\n",
      "           Y       0.60      0.75      0.67         8\n",
      "\n",
      "    accuracy                           0.54        13\n",
      "   macro avg       0.47      0.47      0.46        13\n",
      "weighted avg       0.50      0.54      0.51        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "train_data = year_10_predictions[year_10_predictions['year'] <= 9].dropna(subset=['playoff'])\n",
    "X_train = train_data.drop(columns=['playoff', 'year', 'tmID', 'franchID', 'confID', 'firstRound', 'semis', 'finals', 'rank'])\n",
    "y_train = train_data['playoff']\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "X_test = year_10_predictions[year_10_predictions['year'] == 10].drop(columns=['playoff', 'year', 'tmID', 'franchID', 'confID', 'firstRound', 'semis', 'finals', 'rank'])\n",
    "real_values = teams[teams['year'] == 10]['playoff']\n",
    "\n",
    "X_test = X_test[X_train.columns] \n",
    "\n",
    "dt_predictions = dt_model.predict(X_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(real_values, dt_predictions)\n",
    "dt_precision = precision_score(real_values, dt_predictions, pos_label='Y')\n",
    "dt_recall = recall_score(real_values, dt_predictions, pos_label='Y')\n",
    "dt_f1 = f1_score(real_values, dt_predictions, pos_label='Y')\n",
    "\n",
    "print(\"\\nModel: Decision Tree\")\n",
    "print(f\"Accuracy: {dt_accuracy}\")\n",
    "print(f\"Precision: {dt_precision}\")\n",
    "print(f\"Recall: {dt_recall}\")\n",
    "print(f\"F1 Score: {dt_f1}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(real_values, dt_predictions, target_names=['N', 'Y']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
